{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yichen Wang Assignment 2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orvgBiZaAS-S"
      },
      "source": [
        "# Homework: classify the origin of names using a character-level RNN\n",
        "\n",
        "In this homework we will use an rnn-based model to perform classification. The goal is threefold:\n",
        "\n",
        "1. Get more hands on with the preprocessing needed to perform text classification from A to Z. No preprocessing is done for you!\n",
        "2. Use embeddings and RNNs in conjunction at the character level to perform classification.\n",
        "3. Write a function that takes as input a string, and outputs the name of the predicted class.\n",
        "\n",
        "However, here are guidelines to help you through all the steps:\n",
        "\n",
        "1. Figure out the number of classes, and map the classes to integers (or one-hot vectors). This is needed for fitting the model and training it to do classification.\n",
        "2. Use the keras tokenizer at the character level to tokenize your input into integer sequences.\n",
        "3. Pad your sequences using the keras preprocessing tools.\n",
        "4. Build a model that uses, minimally, an embedding layer, an RNN (of your choice) and a dense layer to output the logits or probabilities for the target classes (name origins).\n",
        "5. Fit the model and evaluate on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxpRYUBTCANr"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF1BqaO7-A2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc4459d-7096-4b63-ae6c-a9b87a00575f"
      },
      "source": [
        "# Download the data\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-16 02:49:08--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.64.174.109, 18.64.174.42, 18.64.174.23, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.64.174.109|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-16 02:49:08 (19.7 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnj5MDEJ-u-q"
      },
      "source": [
        "data = []\n",
        "for filename in glob('data/names/*.txt'):\n",
        "  origin = filename.split('/')[-1].split('.txt')[0]\n",
        "  names = open(filename).readlines()\n",
        "  for name in names:\n",
        "    data.append((name.strip(), origin))\n",
        "\n",
        "names, origins = zip(*data)\n",
        "names_train, names_test, origins_train, origins_test = train_test_split(names, origins, test_size=0.3, shuffle=True, random_state=42)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUhDpvskAHZ1"
      },
      "source": [
        "# Lets look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ux9M4DV-A5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45554d80-c8d0-43e8-d81b-5a75f8dd7802"
      },
      "source": [
        "for name, origin in zip(names_train[:20], origins_train[:20]):\n",
        "  print(name.ljust(20), origin)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pett                 English\n",
            "Hiro                 Japanese\n",
            "Khouri               Arabic\n",
            "Frusher              English\n",
            "Costa                Portuguese\n",
            "Watts                English\n",
            "Khouri               Arabic\n",
            "Slapnickova          Czech\n",
            "Ricchetti            Italian\n",
            "Remeslo              Russian\n",
            "Izumi                Japanese\n",
            "Groisman             Russian\n",
            "Hurrell              English\n",
            "Jangel               Russian\n",
            "Vitoshkin            Russian\n",
            "Bissette             French\n",
            "Juravkov             Russian\n",
            "Hakimi               Arabic\n",
            "Shalahonov           Russian\n",
            "Jeleznyak            Russian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "origins = []\n",
        "for x in origins_train:\n",
        "    if x not in origins:\n",
        "        origins.append(x)\n",
        "print(origins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj-GwmMVxSD-",
        "outputId": "bbc6920b-7109-4c68-f572-4375185cd01c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['English', 'Japanese', 'Arabic', 'Portuguese', 'Czech', 'Italian', 'Russian', 'French', 'Scottish', 'Irish', 'German', 'Greek', 'Dutch', 'Chinese', 'Vietnamese', 'Korean', 'Spanish', 'Polish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Figure out the number of classes, and map the classes to integers (or one-hot vectors). This is needed for fitting the model and training it to do classification.\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow import keras\n",
        "\n",
        "origin_train_encoded = [origins.index(origin) for origin in origins_train]\n",
        "origin_test_encoded = [origins.index(origin) for origin in origins_test]\n",
        "\n",
        "#Use the keras tokenizer at the character level to tokenize your input into integer sequences.\n",
        "# Training set\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(names_train)\n",
        "sequences = tokenizer.texts_to_sequences(names_train)\n",
        "\n",
        "# Test set\n",
        "sequences_1 = tokenizer.texts_to_sequences(names_test)"
      ],
      "metadata": {
        "id": "K6cpVinjxUiM"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pad your sequences using the keras preprocessing tools.\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=None,\n",
        "    dtype='int32',\n",
        "    padding='pre',\n",
        "    truncating='pre',\n",
        "    value=0.0\n",
        ")\n",
        "\n",
        "sequences_1 = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences_1,\n",
        "    maxlen=None,\n",
        "    dtype='int32',\n",
        "    padding='pre',\n",
        "    truncating='pre',\n",
        "    value=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "j6MWmT1nxqX9"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "origin_train_encoded_array = np.array(origin_train_encoded)\n",
        "origin_test_encoded_array = np.array(origin_test_encoded)\n",
        "sequences_array = np.array(sequences)\n",
        "sequences_1_array = np.array(sequences_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfS9UxG5xsCl",
        "outputId": "b48bb527-aead-435b-d0bd-53efa0e6c164"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=len(origin_train_encoded), \n",
        "                           output_dim=embed_size,\n",
        "                           mask_zero=True, # just ignore zeroes instead of learning it\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(len(origins), activation='softmax')\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "i0T9Zv_nxtl7"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(sequences_array, origin_train_encoded_array, epochs=5, validation_data=(sequences_1_array, origin_test_encoded_array))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFY2H2Xax2rM",
        "outputId": "560f7f53-186e-4f45-f1fe-adceccb2d4b9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "471/471 [==============================] - 37s 78ms/step - loss: 0.6251 - accuracy: 0.8087 - val_loss: 0.6069 - val_accuracy: 0.8161\n",
            "Epoch 2/5\n",
            "471/471 [==============================] - 36s 76ms/step - loss: 0.5128 - accuracy: 0.8387 - val_loss: 0.5819 - val_accuracy: 0.8251\n",
            "Epoch 3/5\n",
            "471/471 [==============================] - 36s 76ms/step - loss: 0.4476 - accuracy: 0.8590 - val_loss: 0.5558 - val_accuracy: 0.8338\n",
            "Epoch 4/5\n",
            "471/471 [==============================] - 36s 76ms/step - loss: 0.3895 - accuracy: 0.8737 - val_loss: 0.5430 - val_accuracy: 0.8342\n",
            "Epoch 5/5\n",
            "471/471 [==============================] - 36s 76ms/step - loss: 0.3290 - accuracy: 0.8941 - val_loss: 0.5523 - val_accuracy: 0.8346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_origin(name):\n",
        "  assert isinstance(name, str)\n",
        "  tokenizer.fit_on_texts([name])\n",
        "  x_new = tokenizer.texts_to_sequences([name])\n",
        "  y_proba = model.predict(x_new)\n",
        "  d = dict(enumerate(y_proba.flatten(), 1))\n",
        "  ini_list = ['Arabic', 'Czech', 'Russian', 'English', 'Dutch', 'German', 'Spanish', 'Polish', 'Scottish', 'French', 'Italian', 'Korean', 'Vietnamese', 'Chinese', 'Japanese', 'Greek', 'Irish', 'Portuguese']\n",
        "  # change keys of dictionary from probabilities to origins\n",
        "  final_dict = dict(zip(ini_list, list(d.values())))\n",
        "  # sort the dictionary by largest to smallest probability\n",
        "  sort_final_dict = sorted(final_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "  for i in sort_final_dict:\n",
        "\t  print(i[0], i[1])\n",
        "  # Print out the origin of the name:\n",
        "  x = sort_final_dict[0]\n",
        "  return print(\"The origin of {} is {}.\".format(name, x[0]))"
      ],
      "metadata": {
        "id": "ae0cUdP4yIpu"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_origin(\"inoseke\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEDs7qFUyL31",
        "outputId": "60f65cb0-c4d4-4273-e70f-55d82cc9697f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Japanese 0.96832454\n",
            "Russian 0.028065262\n",
            "Spanish 0.001280578\n",
            "Czech 0.00052627915\n",
            "Italian 0.000523185\n",
            "Portuguese 0.00048513536\n",
            "Polish 0.00032964814\n",
            "Greek 0.00026979213\n",
            "English 4.7200872e-05\n",
            "Dutch 4.3072527e-05\n",
            "Arabic 3.0994757e-05\n",
            "German 2.6547756e-05\n",
            "Irish 2.3635155e-05\n",
            "French 1.8155122e-05\n",
            "Korean 3.310234e-06\n",
            "Scottish 1.2909151e-06\n",
            "Chinese 7.971009e-07\n",
            "Vietnamese 6.4440854e-07\n",
            "The origin of inoseke is Japanese.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_origin(\"putin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqNBOC9T01OG",
        "outputId": "4e1d1a1e-fcdb-449a-e734-4ddf151c296e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Russian 0.6824243\n",
            "Italian 0.2246525\n",
            "Czech 0.028965894\n",
            "German 0.017480794\n",
            "English 0.014840596\n",
            "Vietnamese 0.009190067\n",
            "Chinese 0.005551206\n",
            "Polish 0.0035569298\n",
            "Korean 0.0034208207\n",
            "Irish 0.0028147467\n",
            "Japanese 0.0026763615\n",
            "Spanish 0.0014864838\n",
            "French 0.0013024184\n",
            "Portuguese 0.00092454866\n",
            "Dutch 0.0003513253\n",
            "Greek 0.00024308743\n",
            "Scottish 0.00011644085\n",
            "Arabic 1.530532e-06\n",
            "The origin of putin is Russian.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_origin(\"mohammod\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dQ5OuUX7yNQ",
        "outputId": "9b05cc9c-c748-4e71-9603-1eff9b63035d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic 0.4521369\n",
            "French 0.16324107\n",
            "Portuguese 0.10108853\n",
            "Spanish 0.060494307\n",
            "Dutch 0.04124422\n",
            "German 0.034854922\n",
            "Japanese 0.029759597\n",
            "Russian 0.025759159\n",
            "Irish 0.022234743\n",
            "English 0.020454599\n",
            "Greek 0.016606148\n",
            "Polish 0.016008899\n",
            "Scottish 0.006844208\n",
            "Italian 0.00446331\n",
            "Czech 0.003412573\n",
            "Vietnamese 0.0006495028\n",
            "Chinese 0.00059616676\n",
            "Korean 0.00015117408\n",
            "The origin of mohammod is Arabic.\n"
          ]
        }
      ]
    }
  ]
}