## Project Description
An AlexNet architecture is used to train on the CIFAR100 dataset.

#### Convolutional layer:
A convolution is a mathematical term that describes a dot product multiplication between two sets of elements. Within deep learning the convolution operation acts on the filters/kernels and image data array within the convolutional layer. Therefore a convolutional layer is simply a layer the houses the convolution operation that occurs between the filters and the images passed through a convolutional neural network.

#### Batch Normalisation layer:
Batch Normalization is a technique that mitigates the effect of unstable gradients within a neural network through the introduction of an additional layer that performs operations on the inputs from the previous layer. The operations standardize and normalize the input values, after that the input values are transformed through scaling and shifting operations.

#### MaxPooling layer: 
Max pooling is a variant of sub-sampling where the maximum pixel value of pixels that fall within the receptive field of a unit within a sub-sampling layer is taken as the output. The max-pooling operation below has a window of 2x2 and slides across the input data, outputting an average of the pixels within the receptive field of the kernel.

#### Flatten layer:
Takes an input shape and flattens the input image data into a one-dimensional array.

#### Dense Layer:
A dense layer has an embedded number of arbitrary units/neurons within. Each neuron is a perceptron.
